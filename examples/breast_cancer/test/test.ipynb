{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from zeno import load_data, load_model, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_image = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "classes = (\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")\n",
    "\n",
    "\n",
    "@load_model\n",
    "def load_model(model_path):\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(model_path))\n",
    "    print(net)\n",
    "    \n",
    "    visualisation = {}\n",
    "\n",
    "    def hook_fn(m, i, o):\n",
    "        visualisation[m] = o \n",
    "\n",
    "    def get_all_layers(net):\n",
    "        for name, layer in net._modules.items():\n",
    "            #If it is a sequential, don't register a hook on it\n",
    "            # but recursively register hook on all it's module children\n",
    "            if isinstance(layer, nn.Sequential):\n",
    "                get_all_layers(layer)\n",
    "            else:\n",
    "                # it's a non sequential. Register a hook\n",
    "                layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    get_all_layers(net)\n",
    "\n",
    "    df_metadata = open(\"../extract_subsets_10_10_10/data.csv\", \"r\")\n",
    "    data_path = \"../extract_subsets_10_10_10\"\n",
    "    instances = [PIL.Image.open(os.path.join(data_path, img)) for img in df_metadata.index]\n",
    "\n",
    "    imgs = torch.stack([transform_image(img) for img in instances])\n",
    "    with torch.no_grad():\n",
    "        out = net(imgs)\n",
    "    return visualisation.keys()\n",
    "    \n",
    "    return [classes[i] for i in torch.argmax(out, dim=1).detach().numpy()]\n",
    "\n",
    "    # def pred(instances):\n",
    "    #     imgs = torch.stack([transform_image(img) for img in instances])\n",
    "    #     with torch.no_grad():\n",
    "    #         out = net(imgs)\n",
    "    #     #h.remove()\n",
    "    #     return [classes[i] for i in torch.argmax(out, dim=1).detach().numpy()]\n",
    "    # return pred\n",
    "\n",
    "    # Just to check whether we got all layers\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "@load_data\n",
    "def load_data(df_metadata, data_path):\n",
    "    return [PIL.Image.open(os.path.join(data_path, img)) for img in df_metadata.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m__pycache__\u001b[m\u001b[m/            class.py\n",
      "breast_cancer_tests.py  test.ipynb\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cifar/cifar_net_1.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/umaymahimran/Desktop/zeno/examples/breast_cancer/test/test.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/umaymahimran/Desktop/zeno/examples/breast_cancer/test/test.ipynb#ch0000010?line=0'>1</a>\u001b[0m load_model(\u001b[39m\"\u001b[39;49m\u001b[39mcifar/cifar_net_1.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/zeno/zeno/api.py:7\u001b[0m, in \u001b[0;36mload_model.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      <a href='file:///~/Desktop/zeno/zeno/api.py?line=4'>5</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m      <a href='file:///~/Desktop/zeno/zeno/api.py?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m----> <a href='file:///~/Desktop/zeno/zeno/api.py?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/Users/umaymahimran/Desktop/zeno/examples/breast_cancer/test/test.ipynb Cell 2'\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/umaymahimran/Desktop/zeno/examples/breast_cancer/test/test.ipynb#ch0000001?line=40'>41</a>\u001b[0m \u001b[39m@load_model\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/umaymahimran/Desktop/zeno/examples/breast_cancer/test/test.ipynb#ch0000001?line=41'>42</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(model_path):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/umaymahimran/Desktop/zeno/examples/breast_cancer/test/test.ipynb#ch0000001?line=42'>43</a>\u001b[0m     net \u001b[39m=\u001b[39m Net()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/umaymahimran/Desktop/zeno/examples/breast_cancer/test/test.ipynb#ch0000001?line=43'>44</a>\u001b[0m     net\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(model_path))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/umaymahimran/Desktop/zeno/examples/breast_cancer/test/test.ipynb#ch0000001?line=44'>45</a>\u001b[0m     \u001b[39mprint\u001b[39m(net)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/umaymahimran/Desktop/zeno/examples/breast_cancer/test/test.ipynb#ch0000001?line=46'>47</a>\u001b[0m     visualisation \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py?line=695'>696</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py?line=696'>697</a>\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py?line=698'>699</a>\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py?line=699'>700</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py?line=700'>701</a>\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py?line=701'>702</a>\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py?line=702'>703</a>\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py?line=703'>704</a>\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py?line=228'>229</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py?line=229'>230</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py?line=230'>231</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py?line=231'>232</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py?line=232'>233</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py?line=210'>211</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/torch/serialization.py?line=211'>212</a>\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cifar/cifar_net_1.pth'"
     ]
    }
   ],
   "source": [
    "load_model(\"cifar/cifar_net_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import torch\n",
    "import os\n",
    "from zeno import load_data, load_model, metric\n",
    "from examples.breast_cancer.vgg_old import vgg16_bn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "transform_image = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "classes = (\n",
    "    \"normal\",\n",
    "    \"lesion\"\n",
    ")\n",
    "\n",
    "\n",
    "num_classes = 2\n",
    "image_res = 512\n",
    "transform_image = transforms.Compose(\n",
    "    [transforms.Resize(image_res), transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "@load_model\n",
    "def load_model(model_path):\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = vgg16_bn(pretrained=False)\n",
    "    model.classifier.fc8a = nn.Linear(model.classifier.fc8a.in_features, num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "    # a dict to store the activations\n",
    "    activation = {}\n",
    "    def getActivation(name):\n",
    "        # the hook signature\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    # register forward hooks on the layers of choice\n",
    "    h = model.classifier.drop7.register_forward_hook(getActivation('emb'))\n",
    "    \n",
    "    def pred(instances):\n",
    "        #imgs = torch.stack([transform_image(img) for img in instances], dim=0)\n",
    "        emb = []\n",
    "        for img in instances:\n",
    "            img = transform_image(img)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(img)\n",
    "                emb.append(activation['emb'])\n",
    "                prob = F.softmax(out, dim=1)\n",
    "                print(prob)\n",
    "        print(emb)\n",
    "        return emb\n",
    "        # with torch.no_grad():\n",
    "        #     out = model(imgs)\n",
    "        #     emb.append(activation['emb'])\n",
    "        #     prob = F.softmax(out, dim=1)\n",
    "        # h.remove()\n",
    "        \n",
    "        # return [classes[i] for i in torch.argmax(prob, dim=1).detach().numpy()], np.array(emb)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "@load_data\n",
    "def load_data(df_metadata, data_path):\n",
    "    return [PIL.Image.open(os.path.join(data_path, img)) for img in df_metadata.index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import torch\n",
    "import os\n",
    "from zeno import load_data, load_model, metric\n",
    "from examples.breast_cancer.vgg_old import vgg16_bn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "transform_image = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "classes = (\n",
    "    \"normal\",\n",
    "    \"lesion\"\n",
    ")\n",
    "\n",
    "\n",
    "num_classes = 2\n",
    "image_res = 512\n",
    "transform_image = transforms.Compose(\n",
    "    [transforms.Resize(image_res), transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "@load_model\n",
    "def load_model(model_path):\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = vgg16_bn(pretrained=False)\n",
    "    model.classifier.fc8a = nn.Linear(model.classifier.fc8a.in_features, num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "    visualisation = {}\n",
    "\n",
    "    def hook_fn(m, i, o):\n",
    "        visualisation[m] = o \n",
    "\n",
    "    def get_all_layers(net):\n",
    "        for name, layer in net._modules.items():\n",
    "            #If it is a sequential, don't register a hook on it\n",
    "            # but recursively register hook on all it's module children\n",
    "            if isinstance(layer, nn.Sequential):\n",
    "                get_all_layers(layer)\n",
    "            else:\n",
    "                # it's a non sequential. Register a hook\n",
    "                layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    get_all_layers(model)\n",
    "\n",
    "    df_metadata = pd.read_csv(\"../extract_subsets_10_10_10/data.csv\")\n",
    "    data_path = \"../extract_subsets_10_10_10\"\n",
    "    instances = [PIL.Image.open(os.path.join(data_path, img)) for img in df_metadata.patch_dir]\n",
    "\n",
    "    imgs = torch.stack([transform_image(img) for img in instances])\n",
    "    with torch.no_grad():\n",
    "        out = model(imgs)\n",
    "    print(out)\n",
    "    return visualisation.keys()\n",
    "    \n",
    "\n",
    "@load_data\n",
    "def load_data(df_metadata, data_path):\n",
    "    return [PIL.Image.open(os.path.join(data_path, img)) for img in df_metadata.index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import torch\n",
    "import os\n",
    "from zeno import load_data, load_model, metric\n",
    "from examples.breast_cancer.vgg_old import vgg16_bn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "transform_image = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "classes = (\n",
    "    \"normal\",\n",
    "    \"lesion\"\n",
    ")\n",
    "\n",
    "\n",
    "num_classes = 2\n",
    "image_res = 512\n",
    "transform_image = transforms.Compose(\n",
    "    [transforms.Resize(image_res), transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "@load_model\n",
    "def load_model(model_path):\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = vgg16_bn(pretrained=False)\n",
    "    model.classifier.fc8a = nn.Linear(model.classifier.fc8a.in_features, num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "    # a dict to store the activations\n",
    "    activation = {}\n",
    "    def getActivation(name):\n",
    "        # the hook signature\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    # register forward hooks on the layers of choice\n",
    "    h = model.classifier.drop7.register_forward_hook(getActivation('emb'))\n",
    "    \n",
    "    df_metadata = pd.read_csv(\"../extract_subsets_10_10_10/data.csv\")\n",
    "    data_path = \"../extract_subsets_10_10_10\"\n",
    "    instances = [PIL.Image.open(os.path.join(data_path, img)) for img in df_metadata.patch_dir]\n",
    "    imgs = torch.stack([transform_image(img) for img in instances])\n",
    "\n",
    "    emb = []\n",
    "    with torch.no_grad():\n",
    "        out = model(imgs)\n",
    "        emb.append(activation['emb'])\n",
    "        prob = F.softmax(out, dim=1)\n",
    "        print(prob)\n",
    "    h.remove()\n",
    "    print(emb)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7688, 0.2312]])\n",
      "[tensor([[0.0000e+00, 1.7709e-01, 0.0000e+00, 0.0000e+00, 6.6535e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.4767e-02, 0.0000e+00,\n",
      "         6.1246e-01, 0.0000e+00, 0.0000e+00, 4.5729e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 2.0830e-01, 2.6982e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.3179e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.1529e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 2.1676e-01, 0.0000e+00, 5.3383e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.5019e-01, 0.0000e+00, 7.0137e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 5.2543e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.9100e-01, 0.0000e+00, 1.3836e-01, 0.0000e+00,\n",
      "         5.6841e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7905e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5234e-01,\n",
      "         0.0000e+00, 6.8822e-01, 0.0000e+00, 0.0000e+00, 1.9940e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7709e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9119e-01, 4.1080e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.6404e-01, 0.0000e+00, 0.0000e+00,\n",
      "         4.2888e-01, 5.5599e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2946e-02, 0.0000e+00, 0.0000e+00,\n",
      "         6.7771e-01, 0.0000e+00, 7.1413e-03, 0.0000e+00, 3.1902e-01, 5.3836e-01,\n",
      "         2.1625e-01, 2.7729e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0334e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.6574e-02, 0.0000e+00, 1.0200e-01, 2.4779e-02, 0.0000e+00,\n",
      "         0.0000e+00, 6.3491e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9974e-03,\n",
      "         1.1094e-01, 0.0000e+00, 8.6498e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7828e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7982e-01, 2.8137e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4694e-01, 4.9882e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.6586e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5934e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 8.2701e-01, 2.4928e-02, 2.7167e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1670e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.6151e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.5423e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0833e-01,\n",
      "         0.0000e+00, 0.0000e+00, 1.1173e-01, 4.4679e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 8.0202e-02, 0.0000e+00, 8.9186e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.4584e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.4974e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 8.8862e-01, 2.0180e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.0566e-01, 8.0338e-03, 0.0000e+00, 6.4209e-01,\n",
      "         6.5976e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 8.0650e-02, 0.0000e+00, 0.0000e+00, 4.3161e-01,\n",
      "         0.0000e+00, 7.0135e-01, 0.0000e+00, 0.0000e+00, 4.0897e-01, 0.0000e+00,\n",
      "         0.0000e+00, 5.7155e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.0071e-01, 1.7068e-02, 0.0000e+00, 1.3691e-01, 0.0000e+00, 2.0268e-01,\n",
      "         2.7896e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.5606e-01, 0.0000e+00, 9.5003e-01, 0.0000e+00, 0.0000e+00, 4.1773e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0467e+00, 0.0000e+00, 0.0000e+00, 5.0800e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.3697e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.2718e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3472e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.0786e-01, 0.0000e+00, 0.0000e+00, 2.6038e-04, 0.0000e+00,\n",
      "         4.9781e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.3541e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0777e-02,\n",
      "         0.0000e+00, 0.0000e+00, 1.6750e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4496e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 9.1439e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.2808e-01, 7.3623e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 5.3671e-01, 0.0000e+00, 0.0000e+00,\n",
      "         8.5335e-01, 0.0000e+00, 2.5330e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0626e-01, 0.0000e+00, 0.0000e+00,\n",
      "         2.0652e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.7208e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.2082e-01, 0.0000e+00, 0.0000e+00, 2.0974e-01, 0.0000e+00, 6.2403e-02,\n",
      "         2.0709e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5988e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1177e-02, 0.0000e+00, 0.0000e+00,\n",
      "         6.4654e-02, 0.0000e+00, 1.7929e-01, 5.7177e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 2.3398e-01, 0.0000e+00, 0.0000e+00, 4.5255e-01, 1.8228e-01,\n",
      "         9.3412e-02, 0.0000e+00, 0.0000e+00, 5.0626e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3161e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.8533e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8041e-01,\n",
      "         0.0000e+00, 5.4232e-01]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0000e+00, 1.7709e-01, 0.0000e+00, 0.0000e+00, 6.6535e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.4767e-02, 0.0000e+00,\n",
       "          6.1246e-01, 0.0000e+00, 0.0000e+00, 4.5729e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 2.0830e-01, 2.6982e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.3179e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 4.1529e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 2.1676e-01, 0.0000e+00, 5.3383e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.5019e-01, 0.0000e+00, 7.0137e-02, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 5.2543e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.9100e-01, 0.0000e+00, 1.3836e-01, 0.0000e+00,\n",
       "          5.6841e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7905e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5234e-01,\n",
       "          0.0000e+00, 6.8822e-01, 0.0000e+00, 0.0000e+00, 1.9940e-02, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7709e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9119e-01, 4.1080e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 3.6404e-01, 0.0000e+00, 0.0000e+00,\n",
       "          4.2888e-01, 5.5599e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2946e-02, 0.0000e+00, 0.0000e+00,\n",
       "          6.7771e-01, 0.0000e+00, 7.1413e-03, 0.0000e+00, 3.1902e-01, 5.3836e-01,\n",
       "          2.1625e-01, 2.7729e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          2.0334e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.6574e-02, 0.0000e+00, 1.0200e-01, 2.4779e-02, 0.0000e+00,\n",
       "          0.0000e+00, 6.3491e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9974e-03,\n",
       "          1.1094e-01, 0.0000e+00, 8.6498e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7828e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7982e-01, 2.8137e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4694e-01, 4.9882e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.6586e-02, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5934e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 8.2701e-01, 2.4928e-02, 2.7167e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1670e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.6151e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.5423e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0833e-01,\n",
       "          0.0000e+00, 0.0000e+00, 1.1173e-01, 4.4679e-02, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 8.0202e-02, 0.0000e+00, 8.9186e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 4.4584e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.4974e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 8.8862e-01, 2.0180e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 3.0566e-01, 8.0338e-03, 0.0000e+00, 6.4209e-01,\n",
       "          6.5976e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 8.0650e-02, 0.0000e+00, 0.0000e+00, 4.3161e-01,\n",
       "          0.0000e+00, 7.0135e-01, 0.0000e+00, 0.0000e+00, 4.0897e-01, 0.0000e+00,\n",
       "          0.0000e+00, 5.7155e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          4.0071e-01, 1.7068e-02, 0.0000e+00, 1.3691e-01, 0.0000e+00, 2.0268e-01,\n",
       "          2.7896e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.5606e-01, 0.0000e+00, 9.5003e-01, 0.0000e+00, 0.0000e+00, 4.1773e-02,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.0467e+00, 0.0000e+00, 0.0000e+00, 5.0800e-02, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.3697e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.2718e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3472e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 4.0786e-01, 0.0000e+00, 0.0000e+00, 2.6038e-04, 0.0000e+00,\n",
       "          4.9781e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.3541e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0777e-02,\n",
       "          0.0000e+00, 0.0000e+00, 1.6750e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4496e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 9.1439e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.2808e-01, 7.3623e-02, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 5.3671e-01, 0.0000e+00, 0.0000e+00,\n",
       "          8.5335e-01, 0.0000e+00, 2.5330e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0626e-01, 0.0000e+00, 0.0000e+00,\n",
       "          2.0652e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 2.7208e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          5.2082e-01, 0.0000e+00, 0.0000e+00, 2.0974e-01, 0.0000e+00, 6.2403e-02,\n",
       "          2.0709e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5988e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1177e-02, 0.0000e+00, 0.0000e+00,\n",
       "          6.4654e-02, 0.0000e+00, 1.7929e-01, 5.7177e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 2.3398e-01, 0.0000e+00, 0.0000e+00, 4.5255e-01, 1.8228e-01,\n",
       "          9.3412e-02, 0.0000e+00, 0.0000e+00, 5.0626e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3161e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 4.8533e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8041e-01,\n",
       "          0.0000e+00, 5.4232e-01]])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model(\"../best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    " \n",
    " \n",
    "# Function to convert a CSV to JSON\n",
    "# Takes the file paths as arguments\n",
    "def make_json(csvFilePath, jsonFilePath):\n",
    "     \n",
    "    # create a dictionary\n",
    "    data = []\n",
    "     \n",
    "    # Open a csv reader called DictReader\n",
    "    with open(csvFilePath, encoding='utf-8') as csvf:\n",
    "        csvReader = csv.DictReader(csvf)\n",
    "         \n",
    "        # Convert each row into a dictionary\n",
    "        # and add it to data\n",
    "        for rows in csvReader:\n",
    "             \n",
    "            # Assuming a column named 'No' to\n",
    "            # be the primary key\n",
    "            # key = rows['']\n",
    "            data.append(rows)\n",
    " \n",
    "    # Open a json writer, and use the json.dumps()\n",
    "    # function to dump data\n",
    "    with open(jsonFilePath, 'w', encoding='utf-8') as jsonf:\n",
    "        jsonf.write(json.dumps(data, indent=4))\n",
    "         \n",
    "# Driver Code\n",
    " \n",
    "# Decide the two file paths according to your\n",
    "# computer system\n",
    "csvFilePath = r'breast_cancer/data.csv'\n",
    "jsonFilePath = r'test.json'\n",
    " \n",
    "# Call the make_json function\n",
    "make_json(csvFilePath, jsonFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './patch/version_0/lesion_512/Patient_4222609419/L_MLO/1.135365287044223789913826196762630812825.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/umaymahimran/Desktop/zeno/examples/test.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/umaymahimran/Desktop/zeno/examples/test.ipynb#ch0000009?line=3'>4</a>\u001b[0m     all_test_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/umaymahimran/Desktop/zeno/examples/test.ipynb#ch0000009?line=4'>5</a>\u001b[0m \u001b[39m# print(all_test_data)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/umaymahimran/Desktop/zeno/examples/test.ipynb#ch0000009?line=5'>6</a>\u001b[0m full_img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(PIL\u001b[39m.\u001b[39;49mImage\u001b[39m.\u001b[39;49mopen(all_test_data[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mimaging_dir\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/umaymahimran/Desktop/zeno/examples/test.ipynb#ch0000009?line=6'>7</a>\u001b[0m full_img\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Desktop/zeno/.venv/lib/python3.10/site-packages/PIL/Image.py:2953\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/PIL/Image.py?line=2949'>2950</a>\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/PIL/Image.py?line=2951'>2952</a>\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/PIL/Image.py?line=2952'>2953</a>\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/PIL/Image.py?line=2953'>2954</a>\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/PIL/Image.py?line=2955'>2956</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './patch/version_0/lesion_512/Patient_4222609419/L_MLO/1.135365287044223789913826196762630812825.png'"
     ]
    }
   ],
   "source": [
    "test_data = \"test.json\"\n",
    "import numpy as np\n",
    "with open(test_data, 'r') as f:\n",
    "    all_test_data = json.load(f)\n",
    "# print(all_test_data)\n",
    "full_img = np.array(PIL.Image.open(all_test_data[0]['imaging_dir']))\n",
    "full_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.breast_cancer.vgg_old import vgg16_bn\n",
    "# from skimage.util import view_as_blocks\n",
    "import math\n",
    "import numpy as np\n",
    "# import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from zeno import load_data, load_model, metric\n",
    "\n",
    "num_classes = 2\n",
    "image_res = 512\n",
    "transform_image = transforms.Compose(\n",
    "    [transforms.Resize(image_res), transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "@load_model\n",
    "def load_model(model_path):\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = vgg16_bn(pretrained=False)\n",
    "    model.classifier.fc8a = nn.Linear(model.classifier.fc8a.in_features, num_classes)\n",
    "    model.load_state_dict(torch.load(\"models/breast_cancer/best_model.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "    def pred(instances):\n",
    "        imgs = torch.stack([transform_image(img) for img in instances], dim = 0)\n",
    "        with torch.no_grad():\n",
    "            out = model(imgs)\n",
    "            prob = F.softmax(out, dim=1)\n",
    "        return [classes[i] for i in torch.argmax(prob, dim = 1).detach().numpy()]\n",
    "\n",
    "    return pred\n",
    "\n",
    "@load_data\n",
    "def load_data(df_metadata, data_path):\n",
    "    return [PIL.Image.open(os.path.join(data_path, img)) for img in df_metadata.patch_dir]\n",
    "\n",
    "\n",
    "# def pad_img(img, patch_size, stride):\n",
    "#     h, w = img.shape\n",
    "    \n",
    "#     desired_h = math.ceil( (h - patch_size) / stride ) * stride + patch_size\n",
    "#     desired_w = math.ceil( (w - patch_size) / stride ) * stride + patch_size\n",
    "    \n",
    "#     delta_w = desired_w - w\n",
    "#     delta_h = desired_h - h\n",
    "    \n",
    "#     top, bottom = 0, delta_h\n",
    "#     left, right = 0, delta_w\n",
    "\n",
    "#     color = [0, 0, 0]\n",
    "#     img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "#     return img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_v = 'version_0'\n",
    "\n",
    "# test_data = f'./json/{data_v}/test_{image_res}.json'\n",
    "\n",
    "# with open(test_data, 'r') as f:\n",
    "#     all_test_data = json.load(f)\n",
    "\n",
    "# full_img = np.array(PIL.Image.open(all_test_data[0]['imaging_dir']))\n",
    "\n",
    "# padded_img = pad_img(full_img, image_res, image_res)\n",
    "# patches = view_as_blocks(padded_img, (image_res, image_res)).squeeze()\n",
    "# mask = (np.count_nonzero(patches, axis=(2, 3)) / image_res ** 2) > 0.9\n",
    "# val_inds = np.argwhere(mask > 0)\n",
    "# val_patches = patches[val_inds[:, 0], val_inds[:, 1], ...]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.load_model.<locals>.pred(instances)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model(\"models/breast_cancer/best_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6250e851b2b57b45cf384323290363a857a4c4422319311b44332fe4d33f961d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
