{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from zeno import load_data, load_model, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_image = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "classes = (\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")\n",
    "\n",
    "\n",
    "@load_model\n",
    "def load_model(model_path):\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    def pred(instances):\n",
    "        imgs = torch.stack([transform_image(img) for img in instances])\n",
    "        with torch.no_grad():\n",
    "            out = net(imgs)\n",
    "        return [classes[i] for i in torch.argmax(out, dim=1).detach().numpy()]\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "@load_data\n",
    "def load_data(df_metadata, data_path):\n",
    "    return [PIL.Image.open(os.path.join(data_path, img)) for img in df_metadata.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.load_model.<locals>.pred(instances)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model(\"models/cifar/cifar_net_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    " \n",
    " \n",
    "# Function to convert a CSV to JSON\n",
    "# Takes the file paths as arguments\n",
    "def make_json(csvFilePath, jsonFilePath):\n",
    "     \n",
    "    # create a dictionary\n",
    "    data = []\n",
    "     \n",
    "    # Open a csv reader called DictReader\n",
    "    with open(csvFilePath, encoding='utf-8') as csvf:\n",
    "        csvReader = csv.DictReader(csvf)\n",
    "         \n",
    "        # Convert each row into a dictionary\n",
    "        # and add it to data\n",
    "        for rows in csvReader:\n",
    "             \n",
    "            # Assuming a column named 'No' to\n",
    "            # be the primary key\n",
    "            # key = rows['']\n",
    "            data.append(rows)\n",
    " \n",
    "    # Open a json writer, and use the json.dumps()\n",
    "    # function to dump data\n",
    "    with open(jsonFilePath, 'w', encoding='utf-8') as jsonf:\n",
    "        jsonf.write(json.dumps(data, indent=4))\n",
    "         \n",
    "# Driver Code\n",
    " \n",
    "# Decide the two file paths according to your\n",
    "# computer system\n",
    "csvFilePath = r'breast_cancer/data.csv'\n",
    "jsonFilePath = r'test.json'\n",
    " \n",
    "# Call the make_json function\n",
    "make_json(csvFilePath, jsonFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './patch/version_0/lesion_512/Patient_4222609419/L_MLO/1.135365287044223789913826196762630812825.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/umaymahimran/Desktop/zeno/examples/test.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/umaymahimran/Desktop/zeno/examples/test.ipynb#ch0000009?line=3'>4</a>\u001b[0m     all_test_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/umaymahimran/Desktop/zeno/examples/test.ipynb#ch0000009?line=4'>5</a>\u001b[0m \u001b[39m# print(all_test_data)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/umaymahimran/Desktop/zeno/examples/test.ipynb#ch0000009?line=5'>6</a>\u001b[0m full_img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(PIL\u001b[39m.\u001b[39;49mImage\u001b[39m.\u001b[39;49mopen(all_test_data[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mimaging_dir\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/umaymahimran/Desktop/zeno/examples/test.ipynb#ch0000009?line=6'>7</a>\u001b[0m full_img\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Desktop/zeno/.venv/lib/python3.10/site-packages/PIL/Image.py:2953\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/PIL/Image.py?line=2949'>2950</a>\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/PIL/Image.py?line=2951'>2952</a>\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/PIL/Image.py?line=2952'>2953</a>\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/PIL/Image.py?line=2953'>2954</a>\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/Desktop/zeno/.venv/lib/python3.10/site-packages/PIL/Image.py?line=2955'>2956</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './patch/version_0/lesion_512/Patient_4222609419/L_MLO/1.135365287044223789913826196762630812825.png'"
     ]
    }
   ],
   "source": [
    "test_data = \"test.json\"\n",
    "import numpy as np\n",
    "with open(test_data, 'r') as f:\n",
    "    all_test_data = json.load(f)\n",
    "# print(all_test_data)\n",
    "full_img = np.array(PIL.Image.open(all_test_data[0]['imaging_dir']))\n",
    "full_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.breast_cancer.vgg_old import vgg16_bn\n",
    "# from skimage.util import view_as_blocks\n",
    "import math\n",
    "import numpy as np\n",
    "# import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from zeno import load_data, load_model, metric\n",
    "\n",
    "num_classes = 2\n",
    "image_res = 512\n",
    "transform_image = transforms.Compose(\n",
    "    [transforms.Resize(image_res), transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "@load_model\n",
    "def load_model(model_path):\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = vgg16_bn(pretrained=False)\n",
    "    model.classifier.fc8a = nn.Linear(model.classifier.fc8a.in_features, num_classes)\n",
    "    model.load_state_dict(torch.load(\"models/breast_cancer/best_model.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "    def pred(instances):\n",
    "        imgs = torch.stack([transform_image(img) for img in instances], dim = 0)\n",
    "        with torch.no_grad():\n",
    "            out = model(imgs)\n",
    "            prob = F.softmax(out, dim=1)\n",
    "        return [classes[i] for i in torch.argmax(prob, dim = 1).detach().numpy()]\n",
    "\n",
    "    return pred\n",
    "\n",
    "@load_data\n",
    "def load_data(df_metadata, data_path):\n",
    "    return [PIL.Image.open(os.path.join(data_path, img)) for img in df_metadata.patch_dir]\n",
    "\n",
    "\n",
    "# def pad_img(img, patch_size, stride):\n",
    "#     h, w = img.shape\n",
    "    \n",
    "#     desired_h = math.ceil( (h - patch_size) / stride ) * stride + patch_size\n",
    "#     desired_w = math.ceil( (w - patch_size) / stride ) * stride + patch_size\n",
    "    \n",
    "#     delta_w = desired_w - w\n",
    "#     delta_h = desired_h - h\n",
    "    \n",
    "#     top, bottom = 0, delta_h\n",
    "#     left, right = 0, delta_w\n",
    "\n",
    "#     color = [0, 0, 0]\n",
    "#     img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "#     return img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_v = 'version_0'\n",
    "\n",
    "# test_data = f'./json/{data_v}/test_{image_res}.json'\n",
    "\n",
    "# with open(test_data, 'r') as f:\n",
    "#     all_test_data = json.load(f)\n",
    "\n",
    "# full_img = np.array(PIL.Image.open(all_test_data[0]['imaging_dir']))\n",
    "\n",
    "# padded_img = pad_img(full_img, image_res, image_res)\n",
    "# patches = view_as_blocks(padded_img, (image_res, image_res)).squeeze()\n",
    "# mask = (np.count_nonzero(patches, axis=(2, 3)) / image_res ** 2) > 0.9\n",
    "# val_inds = np.argwhere(mask > 0)\n",
    "# val_patches = patches[val_inds[:, 0], val_inds[:, 1], ...]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.load_model.<locals>.pred(instances)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model(\"models/breast_cancer/best_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6250e851b2b57b45cf384323290363a857a4c4422319311b44332fe4d33f961d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
